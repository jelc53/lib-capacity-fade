\documentclass{article}

\bibliographystyle{plain}
\usepackage[final]{nips_2017}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{comment}
\allowdisplaybreaks
\raggedbottom

\title{Modelling State-of-Health for a Li-ion Battery}
\author{Karthik Nataraj, Hampus Carlens and Julian Cooper}

\begin{document}

\maketitle

\section{Motivation}
Lithium ion batteries are of great and increasing importance in today's society due to their high energy density.  Li-on batteries' performance capability can be characterized by their State of Health (SOH). SOH is a measure of usable capacity over rated capacity. Accurately predicting how many cycles (i.e charge then discharge) a battery can perform, at any given time, before reaching it’s end of useful life is important for reliability. However, prediction is notoriously difficult due to poor understanding of how the measurable parameters (voltage, current, temperature, etc.) effect the SOH. Today three main methods are used: electrochemical, equivalent circuit, and data-driven models.

There have been many previous studies into data-driven models for SOH prediction. Some of the most cited recent publications are \cite{severson2019data}, \cite{roman2021machine} and \cite{energiesMdpi}, all of which produce predictors for RUL with Mean Absolute Percentage Error (MAPE) ranging from 2-15\%.  The lowest errors are obtained using deep neural networks such as in \cite{roman2021machine} and \cite{energiesMdpi}, with MAPE of $~ 2\%$. Multiple authors (including \cite{energiesMdpi} and \cite{DariusOld}) point towards the importance of future work in trying to scale these models, to make them suitable for front-end embedded systems. In \cite{severson2019data} the authors opt for a simple linear model over neural networks. Given information from the first 100 cycles only, they manage to achieve MAPE of 13\% for RUL predictions. 

In this project, we develop a scalable (and explainable) white box model to predict the SOH curve until end-of-life threshold (as opposed to a point estimate of RUL) given the first 100 cycles. Since our predicted SOH curve implies an RUL prediction, we will also compare our implied RUL prediction error to that of previous studies.


\section{Data \& Methodology}
\subsection{Battery cycle data}
The \href{https://data.matr.io/1/projects/5c48dd2bc625d700019f3204}{dataset} we have selected contains approximately 96,700 cycles (approx. 780 cycles per battery for 124 batteries). For each cycle the authors captured voltage, applied current and temperature sampled at 2.5 second intervals. This is the largest publicly available dataset for identical commercial lithium-ion batteries cycled under controlled conditions, and is the same source used by two recent papers that motivated our project: “Machine learning pipeline for battery state-of-health estimation” (Nature, 2021)\cite{roman2021machine} and "Data-driven prediction of battery cycle life before capacity degradation" (Nature, 2019)\cite{severson2019data}. See Appendix 5.1 for exploratory data analysis.

\subsection{Evaluation criteria}
Our goal is to build a model that takes information from the first 100 cycles and predicts State-of-Health (SOH) for the remaining cycles until end-of-life threshold (80\% of nominal). For all model variants, we used ~70\% of the data for training, leaving ~15\% for validation and ~15\% for out-of-sample test. 

To evaluate out-of-sample performance we use two error metrics:
\begin{itemize}
    \item State of Health Curve (SOH): Compute Mean Square Error (MSE) for predicted SOH values, one value per cycle for each battery used in error calculation. %\newline
    \item Remaining Useful Life (RUL): Compute Mean Absolute Percentage Error (MAPE) for predicted RUL values (or equivalently when SOH goes below 80\% of nominal capacity), one value per battery used.
    
\end{itemize}

While these error metrics are related (RUL = first cycle for which SOH dips below end-of-life threshold), they do measure different things. For example, we might correctly predict the number of cycles before end-of-life (let's say 10,000), but guess that the path is linear instead of parabolic or logistic. This might mean that at 50 cycles our prediction of available capacity (SOH) is much worse than the true value despite good RUL accuracy. 

\subsection{Model development}
Our journey towards an effective white box predictor for the SOH curve can be understood as a progression through three model variants. 
\begin{enumerate}
    \item Neural Network: Predicts RUL given information from the first 100 cycles. We built this model to convince ourselves that the model performance from \cite{severson2019data} was reproducible (and equally that we were interpreting their engineered feature variables correctly), and perform feature importance analyses to identify which variables might be most valuable for SOH prediction in subsequent models. \newline

    \item Time Series Models: Our first attempt to produce a white box model of the SOH curve, rather than RUL point prediction. We tried auto-regression and exponential smoothing methods, but both suffered from the same issue: we could not effectively "pool" data across batteries, leading to poor performance. \newline

    \item Bayesian Inference Model: Our second idea for building a white box model of the SOH curve involved imposing the known physics of our problem on our model specification. We know that battery discharge capacity has an exponential decay relationship with cycle number, and a fixed y-asymptote at the nominal capacity. Therefore, we can impose a functional form that reflects this physical behaviour and only ask our model to learn the unknown parameters of this function.
\end{enumerate}

\section{Experiments \& Discussion}
The following sections discuss the design choices and results from each of the models we developed in pursuit of a white box predictor for the SOH curve. 

\subsection{Neural Network}
STATE UPFRONT GOAL OF THIS MODEL: TO PREDICT RUL USING FEATURES FROM PAPER
(1) REPRODUCE RESULTS, (2) FEATURE ENGINEERING / IMPORTANCE

Interestingly in \cite{severson2019data} only a regularized elastic net was used to predict remaining useful life of a set of batteries, using information gleaned from the first 100 cycles.  Our first step was to use a similar feature set but using a black-box neural network instead, to see if this more complex model could obtain a lower MAPE.  (Our primary metric for this task was MAPE as that was what was used in the \cite{severson2019data} paper, so this provided a similar basis for comparison).  In an effort to assess an optimal architecture for a shallow neural network we performed a random search hyperparameter tuning experiment initially on the learning rate (sampling from .01, .001, .0001) and number of neurons in each of two hidden layers (first layer ranging from $\approx 500 - 2000$ nodes, second from $\approx 0 - 500$).  The learning rate range seemed reasonable as per the experiment in \cite{kerastuner} and ranges for the nodes of the hidden layer covered a reasonably wide span, and was also similar to the successful architecture proposed in \cite{roman2021machine}.  Our starting feature set was original set of 20 features proposed in Supplementary Note 1 of \cite{severson2019data}, along with average temperature and current for the first 100 battery cycles.  \newline
However, since $n = 81$ was so small the optimal parameters that the tuning algorithm selected were not optimal on the test set, in particular the learning rate chosen by the tuning procedure was .01 while a rate of .001 performed almost $5 \%$ better on the test set. Therefore further tuning would not be helpful and we used the network architecture determined by the preliminary tuning, consisting of 2 hidden layers, first having 1076 neurons and the second 96.  We then used the learning rate of .001 with the Adam optimizer, ReLU activation, and full-batch gradient descent (since the training dataset is already small).  After noticing a slow training convergence and poor test performance even with l1/l2 regularization, we further condensed the original feature set using shapley values, following the work in \cite{shap}, to derive a less noisy model based on the 3 features with the highest mean feature importance scores (see (a) of \ref{fig:birds}). Our final model yielded a MAPE of $\approx 10.8 \%$, outperforming the best model in the paper which obtained a MAPE of $13 \%$. (b) in Figure \ref{fig:birds} below visually shows the predictions vs. actuals:


\begin{figure}[H]
\captionsetup{font=footnotesize,labelfont={bf}}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth,height = 5cm]{figs/shap.png}
         \caption{\textbf{Top 9 feature importance scores for non time-series variables; Refer to \cite{severson2019data} for variable formulae}}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth,height = 5cm]{figs/obspred.png}
         \caption{\textbf{Predicted vs. Actual cycle lives}}
         \label{fig:three sin x}
     \end{subfigure}
\caption{Descriptive plots for neural network approach to Predicting cycle life}
\label{fig:birds}
\end{figure} 

\subsection{Time Series Models}
STATE GOAL UPFRONT: TO PREDICT THE CURVE FOR EACH BATTERY USING INSIGHT FROM NNET 

% provide (1) purpose / goal of that model type, (2) design choices made (eg. multiplicative trend, choice of p, q, d, tuning hyper params such as number of training cycles), and (3) results and output plot
The purpose of using time series models was to predict the whole capacity degradation curve, not only the RUL as with the neural net. ARIMA (Autoregressive integrated moving average) models can be used with or without exogenous variables. An exogenous variable is in our case a parallel time series of known values that can be imposed on the model/used as a weighted input. Using an ARIMA model without exogenous variables to predict the capacity degradation curve we only considered one measurement per cycle, namely the discharge capacity $Q_D$. From a machine learning perspective we had a separate model for each battery and therefore there was no cross-learning between batteries. This left us with a very limited amount of data to learn from, specifically only one data point per cycle. Forecasting short, non-seasonal time series is difficult. A previous study \cite{timeSeries}, suggests using more advanced methods to overcome the data scarcity problem. Additionally, as the intended outcome was to predict RUL and degradation curve shape from only the first 100 cycles it became apparent that the data scarcity was severe (we were left with only 100 data points to learn from). It quickly became clear that this was impossible. Therefore, we tried predicting using more than the first 100 cycles, specifically a train/test split of 0.7/0.3 (indicating the fractions of train and test data) corresponding to prediction on 400+ cycles. 

The ARIMA model consists of three parts corresponding to three defining parameters, p, d, q, where p = number of lags used, d = degree of differencing and q = order of moving average. Implementing the ARIMA model we set these parameters using two different methods; "auto arima" which performs an automatic search for the best parameters based on an information criterion e. g AIC or BIC and, the Box-Jenkins method which is based on the properties of the ACF and PACF. 

Using auto ARIMA from pmdarima \cite{pmdarima}, a train/test split of 0.7/0.3 and no exogenous variables we got the results presented figures (\ref{fig:sub1}), (\ref{fig2:sub1}) and (\ref{fig2:sub2}). From these results, we could conclude that the ARIMA model is more or less useless if used as is. However, we also tried adding exogenous variables to the ARIMA model. This created the possibility of achieving cross-learning between batteries as we could use the full degradation data from one battery as an exogenous variable when predicting on another battery. It gave a lot more data to predict on and knowledge about expected future behavior. Using the same discharge capacity data (one measurement per cycle) and one exogenous variable series it can be seen in figures (\ref{fig:sub2})-(\ref{fig:sub6}) that the predictions clearly improve when adding the exogenous variables. One can even argue that the predictions are somewhat reasonable. The exogenous variables with the greatest performance enhancement were "IR" (internal resistance) and "QD" (discharge capacity). There was no clear advantage of adding more than one exogenous variable, therefore the results using two exogenous variables are left out. It should be pointed out that the predictivity of the exogenous parameters could be questioned as it was somewhat random when they increased performance significantly, but, they never decrease performance. However, in general, the predictions using anything less than a 0.7/0.3 train/test split were seemingly random and much dependent on from which other battery the exogenous variable came. In some cases e.g see figure (\ref{fig:sub6}) great predictions where possible using as much as a 0.5/0.5 train/test split.

An attempt was made to improve the results by manually setting the ARIMA parameters using Box-Jenkins method. However, the results did, not surprisingly, not become better and the plots are therefore left out.

\begin{figure}[H]
\centering
    \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_ARIMA_no_exog_03split_b1c0.png}
  \caption{b1c0 no exog}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_arima_exog(c1-IR)_03split_b1c0.png}
  \caption{b1c0 with IR from b1c1 as exog}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_arima_exog(c2-IR)_03split_b1c0.png}
  \caption{b1c0 with IR from b1c2 as exog}
  \label{fig:sub3}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_arima_exog(c1-QD)_03split_b1c0.png}
  \caption{b1c0 with QD from b1c1 as exog}
  \label{fig:sub4}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_arima_exog(c2-QD)_03split_b1c0.png}
  \caption{b1c0 with QD from b1c2 as exog}
  \label{fig:sub5}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_arima_exog(c1-QD)_05split_b1c0.png}
  \caption{b1c0 with QD from b1c2 as exog and 0.5 split}
  \label{fig:sub6}
\end{subfigure}
\label{fig:auto_ARIMA_no_exog}
\end{figure}

\begin{figure}[H]
\centering
    \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_arima_no_exog_03split_b1c1.png}
  \caption{b1c1 no exog}
  \label{fig2:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_arima_no_exog_03split_b2c0.png}
  \caption{b2c0 no exog}
  \label{fig2:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_arima_exog(c4-QD)_03split_b2c0.png}
  \caption{b2c0 with QD from b2c4 as exog}
  \label{fig2:sub3}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/auto_arima_exog(c4-IR)_03split_b2c0.png}
  \caption{b2c0 with IR from b2c4 as exog}
  \label{fig2:sub3}
\end{subfigure}
\end{figure}

We also tried a time series model called "exponential smoothing", but, the performance was not satisfactory. The Holt model performed better than the Holt-winter method, see figure (\ref{fig:exp_holt}) for example performance of the Holt model. Considering the much worse performance than the ARIMA models the exponential smoothing models were disregarded as predictors.

\begin{figure}[H]
\centering
    \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/holt_exp_smooth_split07.png}
  \caption{b1c0 holt exp smoothing}
  \label{fig:exp_holt}
\end{subfigure}%
\end{figure}


\subsection{Bayesian Inference Model}
Given the limited performance of our time series model variants, we explored Bayesian Inference as a way to impose more structure (known physics) on the problem. Main idea: since we know the discharge capacity curve of each battery must be a decay curve, why don't we specify such a functional form and only ask our model to learn the shape and translation parameters. Another anticipated benefit of this approach was that we would be able to effectively pool (and learn from) data across batteries - a significant limitation of our ARIMA and Exponential Smoothing models.

In constructing our Bayesian Inference Model we made a number of design choices, including (1) functional form, (2) parameterization and (3) selection of priors.

\begin{enumerate}
    \item \textbf{Function form}: We investigated two different functional forms that described the physical behaviour we would expect in our region of interest: (a) shifted exponential decay and (b) inverse sigmoid.

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/example_exponetial_decay.png}
            \caption{Shifted exponential decay}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/example_inv_sigmoid.png}
            \caption{Inverse sigmoid}
        \end{subfigure}
        \caption{Comparison of functional forms fit to data for an example battery cell}
        \label{fig:funcforms}
    \end{figure}

    While the exponential decay curve seemed like a more natural fit (no inflexion point where curvature reverses), we found that the inverse sigmoid also described the data well in our region of interest (y = 0.8 to 1.2) and its parameters were more interpretable. For example, in our inverse sigmoid formulation, $\alpha$ corresponds to shape (or rate of decay), $\beta$ translates to horizontal translation of our sigmoid midpoint, and $\gamma$ is our y-asymptote (which we can set equal to our nominal discharge capacity for each battery). In contrast, for the exponential decay formulation, changing $\alpha$ or $\beta$ will both affect shape and horizontal translation. 
    % This leads to poor convergence of our joint posterior but also restricts our ability to interrogate what the model has learned.  

    \item \textbf{Parameterization}: Having selected a functional form, we then needed to decide how to relate our function parameters $\alpha$ (rate of decay), $\beta$ (translation) and $\gamma$ (y-asymptote) to our data features (inspired by neural network variable importance from Section 3.1). 

    \begin{itemize}
        \item \textbf{$x_1$}: nominal discharge capacity (= Qd after first cycle)
        \item \textbf{$x_2$}: variance between cycles 10 and 100 of Qd difference as a function of voltage 
        \item \textbf{$x_3$}: log of magnitude of the minimum of the Qd difference
        \item \textbf{$x_4$}: average charge time for cycles 2 through 6
        \item \textbf{$x_5$}: sum of average temperature for cycles 2 through 100
    \end{itemize}

    We choose linear models for $\alpha$ and $\beta$ based on the effectiveness of the linear model from \cite{severson2019data} for predicting RUL (closely related to translation in our case). Then for $\gamma$ we further restrict our formulation by specifying that the y-asymptote must equal the nominal discharge capacity ($x_1$).
    \begin{align*}
        \alpha & = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 + a_5 x_5 && \text{linear model for rate of decay}\\
        \beta & = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 + b_4 x_5 + b_5 x_5 && \text{linear model for horizontal translation}\\
        \gamma & = x_1 && \text{asymptote set to nominal discharge capacity}
    \end{align*}

    \item \textbf{Prior specification}: We make an assumption that our labels $y$ (discharge capacity for each cycle for each battery) are generated from a normal distribution with mean $\hat{y}$ (our predictions based on learned parameters) and variance $\sigma^2$ (which we learn as a parameter).
    $$ y \sim \text{Normal}\left(\gamma - \frac{1}{1+\exp(-\alpha(x-\beta))}, \quad \sigma^2\right)$$

    For our model parameters, we impose informative priors on $a_0, b_0$ based on aggregate analysis of our data: $a_0 \sim N(3,1)$ and $b_0 \sim N(2,1)$, and weakly informative (standard normal) priors on the remaining $a_i, b_i$ parameters since we did not have pre-existing intuition for these relationships. Finally, for variance, we impose a more traditional gamma prior $\sigma^2 \sim \text{Gamma}(1, 2)$.

\end{enumerate}

To sample from our joint posterior, we use the Hamiltonian Monte Carlo No U-Turn sampling method (Stan in-built). See Appendix 5.2 for detailed analysis of posterior marginals and sampling efficiency. \newline

\textbf{Model evaluation}: On out-of-sample test data, our model achieves Mean Square Error of [xx] for SOH prediction and Mean Absolute Percentage Error of [yy] for RUL prediction. [...]

[PLACEHOLDER: INSERT EXAMPLE PREDICTION PLOTS WITH ERROR REGIONS]
    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/bayes_plot_with_error_b1c0.png}
            \caption{SOH MSE = [xx], RUL MAPE = [yy]}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/bayes_plot_with_error_b1c0.png}
            \caption{SOH MSE = [xx], RUL MAPE = [yy]}
        \end{subfigure}
        \caption{Predicted discharge capacity over normalized cycle life}
        \label{fig:bayespred}
    \end{figure}

\section{Conclusions}
Next step: refine parameterization, ie. more physics intuition might lead us to not assume a linear model for alpha or beta

...

...




\section{Appendix}
\subsection{Data exploration}

\textbf{Discharge capacity vs cycle number}. Our goal is to predict the discharge curves below given information from the first 50-100 cycles. A few things to note. First, the batteries in our population are either rated at 1.1 Ah or 1.05 Ah nominal capacity. By nominal here we mean manufacturer rated initial capacity. Second, in practice, our initial discharge capacity rarely perfectly matches the nominal capacity rating and so instead of two discrete starting points at 1.1 and 1.05, our starting points range continuously in that range. Third, our data is meant to reflect cycling each battery from its initial capacity to 80\% of its nominal. This explains the two distinct end points we observe in the right hand chart at 0.88 Ah and 0.84 Ah, the 80\% thresholds of 1.1 Ah and 1.05 Ah nominal capacities respectively. Last, in plotting these curves we identified some obvious outliers. Batteries in the left hand plot with capacities either above 1.2 or below 0.8 are erroneous measurements that we remove from the data before training.  

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/discharge_capacity_by_cycle.png}
            \caption{Complete dataset}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/discharge_capacity_by_cycle_remove_outliers.png}
            \caption{Outliers removed}
        \end{subfigure}
        \caption{Discharge capacity by cycle number for 124 batteries}
        \label{fig:3a}
    \end{figure}

\textbf{Distribution of our target variable}. Cycle life is the number of cycle it takes for a given battery to reach 80\% of its nominal capacity. This is our target variable. When plotting the complete dataset we identified a skewed normal distribution. Ideally we want to roughly maintain this distribution for our validation and test data. To achieve this we re-used logic from "Data-driven prediction of battery cycle life before capacity degradation" \cite{severson2019data} to split our train, validation and test data.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5] {figs/histogram_cycle_life_traintest.png}
        \caption{Distribution of target for train, validation and test data}
        \label{fig:1b}
    \end{figure}

\textbf{Applied current and charge cycle}. The charts below illustrate current, charge capacity, and  discharge capacity for all cycles of an example battery data point (ref: b1c0). We can immediately confirm that the vast majority of cycles follow similar charge profiles. In particular, applying positive current (charging) for first 500-700 time steps, then switching to an applied negative current (discharging) until depleted at the end of the cycle. The charge capacity (Qc) and discharge capacity (Qd) charge plots also show this transition, with Qc increasing up until the changeover to negative applied current, and Qd increasing only after the changeover.

        \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.32\linewidth}
            \includegraphics[width=\linewidth]{figs/b1c0_iapp_intracycle.png}
            \caption{Applied current}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\linewidth}
            \includegraphics[width=\linewidth]{figs/b1c0_qc_intracycle.png}
            \caption{Charge capacity (Qc)}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\linewidth}
            \includegraphics[width=\linewidth]{figs/b1c0_qd_intracycle.png}
            \caption{Discharge capacity (Qd)}
        \end{subfigure}
        \caption{Charge and discharge cycles for an example battery (ref: b1c0)}
        \label{fig:1c}
    \end{figure}

We also investigated how voltage and temperature vary over the cycles of the same example battery. It is interesting to note that while temperature has a roughly gaussian distribution throughout any given cycle, the voltage measure has almost no variance during charge but significant variance across cycles during discharge. 

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/b1c0_voltage_intracycle.png}
            \caption{Voltage (volts)}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/b1c0_temp_intracycle.png}
            \caption{Temperature (degrees celsius)}
        \end{subfigure}
        \caption{Voltage and temperature over cycles for an example batter (ref: b1c0)}
        \label{fig:1d}
    \end{figure}
    
\textbf{Correlation with cycles @ 5\% fade}. Finally, we wanted to directly plot some measure of capacity fade (in this case number of cycles to reach 5\% decrease from nominal) during the initial cycles against cycle life to see if we can recover the correlated behaviour we expect between early trajectory and end point of the discharge capacity curve. Encouragingly, we see the two are highly correlated, achieving a pearson correlation coefficient of approximately 0.94.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5] {figs/correlation_cycle_life_vs_5pct_fade.png}
        \caption{Scatter plot demonstrating correlation between cycle life and 5\% capacity fade}
        \label{fig:1e}
    \end{figure}


\subsection{Hamiltonian Monte Carlo Sampling}

\textbf{Sampled posterior marginals}. Our sampled posterior marginal histograms are what we use to make predictions. We can take the mean from each (converges to MLE for large training dataset) or use information from these distributions to generate predictions that reflect prediction uncertainty (eg. +/- standard deviation). 

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/bayes_alpha_sampled_histogram.png}
            \caption{Rate of decay parameters ($\alpha$)}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/bayes_beta_sampled_histogram.png}
            \caption{Translation parameters ($\beta$)}
        \end{subfigure}
        \caption{Sampled posterior marginals for alpha and beta linear model parameters}
        \label{fig:bayeshist}
    \end{figure}

\textbf{Sampling efficiency}. Our trace plots (only samples after burn-in shown) give a sense of the path that our MCMC sampler (No U-turn Hamilton Monte Carlo) took through the posterior space. AS we had hoped, these show reasonable variation, but no major divergences or long periods of repeated value where the sampler was stuck.

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/bayes_alpha_sampled_traceplot.png}
            \caption{Rate of decay parameters ($\alpha$)}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[width=\linewidth]{figs/bayes_beta_sampled_traceplot.png}
            \caption{Translation parameters ($\beta$)}
        \end{subfigure}
        \caption{Trace plot of HMC sampling for alpha and beta linear model parameters}
        \label{fig:bayestrace}
    \end{figure}

When considering sampling efficiency we also consider auto-correlation between samples. Ideally, we want to take gradient steps through the posterior space such that we maximize the number of "effective" (uncorrelated) samples. In this case, from 1,000 draws we generate ~175 effective samples.

\subsection{GDA, an alternative model}
We built a GDA model for RUL/cycle life predictions as a comparison to the neural net and because it had not, to our knowledge, been done before. Furthermore, it could also be reasonable to believe that the different batteries come from some normal distributions with respect to cycle life. We created one class per 100th increase in cycle life, i. e 19 separate classes, equally spaced between 100 and 1900, and tried to assign each battery in the test set to one of these classes. The model took 27 different features per cycle for each battery and tried to classify the cycle life based on the first 100 cycles. The features were not scaled. As labels, we used the known cycle life of each battery. Specifically we used an LDA (linear discriminant analysis) model implemented using the SKlearn LinearDiscriminantAnalysis class \cite{scikit-learn}. The results are presented in \ref{fig:GDA_scatter}. We notice an accuracy similar to the neural network with a MAPE of $13.14\%$. However, it should be emphasized that we did predictions on discrete classes and not on the exact cycle life.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figs/GDA scatter plot.png}
    \caption{GDA results scatter plot}
    \label{fig:GDA_scatter}
\end{figure}

\bibliography{citation}
\end{document}